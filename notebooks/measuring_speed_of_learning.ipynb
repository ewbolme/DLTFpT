{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Learning in Intermediate Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we take a simple fully connected model for classifying MNIST digits and we examine the speed of learning at various hidden layers within the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jonkrohn/DLTFpT/blob/master/notebooks/measuring_speed_of_learning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout # new!\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a custom callback\n",
    "\n",
    "This callback grabs the weights at the end of each epoch and converts them back to gradients. The gradients are saved in a dictionary and at the end of training the Euclidean norm is taken as a measure of how fast training is moving in any given layer during any given epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradHistory(tensorflow.keras.callbacks.Callback):\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        # Initialize list of hidden layers\n",
    "        self.layers = [layer for layer in self.model.layers if 'hidden_' in layer.name]\n",
    "        \n",
    "        # Initialize grads dict with empty list\n",
    "        self.grads = {layer.name:[] for layer in self.layers}\n",
    "        \n",
    "        # Grab the initial weights from the model\n",
    "        self.prev_w = [layer.get_weights()[0] for layer in self.layers]\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        # Get the weights at the end of the epoch\n",
    "        curr_w = [layer.get_weights()[0] for layer in self.layers]\n",
    "        \n",
    "        # Get the LR at the end of the epoch\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        \n",
    "        # Convert the previous and currents weights to gradients\n",
    "        grads_ = [(prev - curr) for curr, prev in zip(curr_w, self.prev_w)]\n",
    "        \n",
    "        # Move the grads into the self.grads dict\n",
    "        for i,layer in enumerate(grads_):\n",
    "            self.grads[self.layers[i].name].append(layer)\n",
    "        self.prev_w = curr_w\n",
    "    \n",
    "    def on_train_end(self, logs={}):\n",
    "        # At the end of training, take the euclidean norm of each array of gradients in each layer at each epoch.\n",
    "        self.norms = {k:[np.sqrt(np.sum([x*x for x in epoch])) for epoch in v] for k,v in self.grads.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784).astype('float32')\n",
    "X_test = X_test.reshape(10000, 784).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "y_train = to_categorical(y_train, n_classes)\n",
    "y_test = to_categorical(y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design neural network architecture\n",
    "\n",
    "This function builds and returns a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hidden = 2):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, activation='sigmoid', input_shape=(784,),name='input'))\n",
    "    \n",
    "    for h in range(hidden):\n",
    "        model.add(Dense(30, activation='sigmoid',name='hidden_{}'.format(h)))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build, compile and train model\n",
    "\n",
    "Here we actually call the `build_model()` function, compile the model, and return the history object (where the gradients will besaved after training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(hidden = 1, epochs=200):\n",
    "    \n",
    "    model = build_model(hidden)\n",
    "    history = GradHistory()\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "    model.fit(X_train[:1000], y_train[:1000], batch_size=1, epochs=epochs, verbose=0, callbacks=[history])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`history.grads.norms` is a dict, with keys for each of the layers in the model. Each key contains a list of gradient norm values for that layer over all the epochs in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the method to build and train the model, returning the history object\n",
    "# Note, here we use five hidden layers and 200 epochs\n",
    "norms_5 = build_and_train(5, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(history, hidden, log=False):   \n",
    "    fig = plt.figure(figsize=(8,6),dpi=300)\n",
    "    \n",
    "    layers = [layer for layer in history.norms.keys()]\n",
    "    values = [history.norms[layer] for layer in layers]\n",
    "    \n",
    "    for layer,values in zip(layers[::-1],values[::-1]):\n",
    "        ys = np.array(values[:])\n",
    "        xs = np.array(range(ys.shape[0]))\n",
    "        ys_smooth = gaussian_filter1d(ys, sigma=3)\n",
    "        plt.plot(xs, ys_smooth, label=layer)\n",
    "        \n",
    "    plt.title('Learning speed with {} hidden layers'.format(hidden))\n",
    "    plt.ylabel('Learning speed')\n",
    "    if log:\n",
    "        plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(norms_5, 5, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
